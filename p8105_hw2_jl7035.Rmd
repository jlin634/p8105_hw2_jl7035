---
title: "P8105_hw2_jl7035"
author: "Jeffrey Lin"
date: "2024-10-02"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Necessary Libraries
```{r}
library(tidyverse)
library(readr)
```

# Problem 1

## Load Transit Data
```{r}
NYC_Transit_df <- read_csv(
  "Local_Data_Files/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")

NYC_Transit_df <- 
  NYC_Transit_df %>%
  janitor::clean_names()
  

head(NYC_Transit_df)

```

## Inspect Characteristics of the Data Frame
```{r}
nrows_NYC <- nrow(NYC_Transit_df)
nrows_NYC

ncols_NYC <- ncol(NYC_Transit_df)
ncols_NYC

col_names_NYC <- colnames(NYC_Transit_df)
col_names_NYC

```
## Selecting the Relevant Columns in the Data Set
```{r}
NYC_Transit_df <- NYC_Transit_df %>%
  select(line, station_name, station_latitude, station_longitude,
         route1:route11, entry, vending, entrance_type, ada)

head(NYC_Transit_df)

NYC_Transit_df <- NYC_Transit_df %>% 
  mutate(across(route8:route11, as.character))
         
head(NYC_Transit_df)
         
```

This dataset contains various information regarding NYC transit. In it's 
current format, after doing a few steps of cleaning, the dimensions of the 
dataset are `r dim(NYC_Transit_df)`. The current variables of interest, based 
on the columns that have been selected are as follows: 
`r colnames(NYC_Transit_df)`. They broadly cover the location of stations, 
as well as the trains that come through each station, and each respective 
station's accessibility. In order to clean this dataset, I first read in the 
data into a dataframe and inspected it with head(), as well as looked at its 
dimensions and column names. Afterwards, I selected the instructed columns and 
once again inspected it through head. There, I noticed that not all route 
columns were standardized to char type, so I did such. After all these steps,
I would still not say this data is tidy. I see some potential issues with 
how the route data is being represented. Looking at it, I wonder if it would 
be better to condense the data into two columns: (1) The route number, (2)
the train on that route. This way, we could avoid large amounts of NA values 
for route numbers that simply don't exist for certain stations.

## Determine number of distinct stations
```{r}
NYC_Transit_df <- NYC_Transit_df %>%
  distinct(line, station_name, .keep_all =  TRUE)

```
There are `r nrow(NYC_Transit_df)` distinct stations in NYC.


## Determine number of ADA Compliant Stations
```{r}
NYC_Transit_df <- NYC_Transit_df %>% 
  filter(ada = TRUE)

```
There are `r nrow(NYC_Transit_df)` ADA compliant stations.


## What proportion of station entrances / exits without vending allow entrance?
```{r}
Prop_Allow_Entrance <- NYC_Transit_df %>% 
  filter(
    vending =="NO", 
    .keep_all = TRUE
  ) %>%
  pull(entry) %>% 
  case_match(
    "YES" ~ 1, 
    "NO" ~0
  ) %>%
  mean()
  


```
The proportion of station entrances/exists without vending machines, that allow
entrances is `r Prop_Allow_Entrance`

## Reformat data so that route number and route name are distinct variables
```{r}
NYC_Transit_df <- NYC_Transit_df %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  )

head(NYC_Transit_df)
```



## Determine number of distinct stations serving the A train
```{r}
Serve_A <- NYC_Transit_df %>% 
  filter(route_name == "A") %>%
  select(line, station_name) %>%
  distinct()

```
There are `r nrow(Serve_A)` unique stations serving the A train.

## Determine how many of the stations serving the A train are ADA Compliant
```{r}
Station_A_compl <- NYC_Transit_df %>% 
  filter(route_name == "A", 
    ada = TRUE
  ) %>%
  distinct(line, station_name)


```
There are `r nrow(Station_A_compl)` stations which serve the A train and are 
ADA Compliant.





# Problem 2

## Read in Mr. Trash Wheel Data
```{r}
Mr_Trash_Wheel_df <- 
  readxl::read_excel(
    "Local_Data_Files/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel", 
    na = c("NA", ".", ""),
    range = "A2:N653"
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    Trash_Wheel_Type = "Mr. Trash Wheel")

head(Mr_Trash_Wheel_df)

```



## Round sports ball to nearest integer and convert type to integer
```{r}
Mr_Trash_Wheel_df <- Mr_Trash_Wheel_df %>% 
  mutate(
    sports_balls = as.integer(round(sports_balls))
  ) 

head(Mr_Trash_Wheel_df)

```

## Convert Year from char type to double
```{r}
Mr_Trash_Wheel_df <- Mr_Trash_Wheel_df %>% 
  mutate(year = as.double(year))


```


## Read in Professor Trash Wheel Data
```{r}
Professor_Trash_Wheel_df <-
  readxl::read_excel(
    "Local_Data_Files/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    na = c("NA", ".", ""),
    range = "A2:M120",
  ) %>%
  janitor::clean_names() %>%
  mutate(
    Trash_Wheel_Type = "Professor Trash Wheel", 
    sports_balls = NA
  ) %>% 
  relocate(sports_balls, .before = homes_powered
  )

head(Professor_Trash_Wheel_df)

```
## Round homes_powered to nearest integer and convert type to integer

```{r}
Professor_Trash_Wheel_df <- Professor_Trash_Wheel_df %>% 
  mutate(homes_powered = as.integer(round(homes_powered)))

head(Professor_Trash_Wheel_df)

```

## Read in Gwynnda Trash Wheel Data Set

```{r}
Gwyn_Trash_Wheel_df <- 
  readxl::read_excel("Local_Data_Files/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  na = c("NA", ".", ""),
  range ="A2:L265"
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    Trash_Wheel_Type = "Gwynnda Trash Wheel",
    sports_balls = NA, 
    glass_bottles = NA
  ) %>% 
  relocate(sports_balls, .before = homes_powered) %>% 
  relocate(glass_bottles, .before = plastic_bags)

head(Gwyn_Trash_Wheel_df)

```

## Round homes_powered to nearest integer and convert type to integer
```{r}
Gwyn_Trash_Wheel_df <- Gwyn_Trash_Wheel_df %>% 
  mutate(homes_powered = as.integer(round(homes_powered)))

head(Gwyn_Trash_Wheel_df)

```

## Bind 3 datasets together 

```{r}

Trash_Wheel_df <- 
  bind_rows(Mr_Trash_Wheel_df, Professor_Trash_Wheel_df, Gwyn_Trash_Wheel_df)

head(Trash_Wheel_df)

```

After combining the datasets, we find that there are `r nrow(Trash_Wheel_df)`
total observations in this dataset. The key variables to note in this dataset 
include date, weight in tons, volume, number of plastic bottles, number of 
cigarette butts, number of glass bottles, number of plastic bags, number of 
wrappers, and Trash Wheel Type. The total weight of trash collected by 
Professor Trash Wheel is `r Trash_Wheel_df %>% filter(Trash_Wheel_Type == 
"Professor Trash Wheel") %>% pull(weight_tons) %>% sum()` tons. The total 
number of cigarette butts collected by the gwynnda trash wheel in June of 2022 
is `r Trash_Wheel_df %>% filter(month == "June", year == 2022, 
Trash_Wheel_Type == "Gwynnda Trash Wheel") %>% pull(cigarette_butts) %>% sum()`.


# Problem 3


## Reading in and cleaning bakers csv file
```{r}
bakers_df <- 
  read_csv(
    "Local_Data_Files/gbb_datasets/bakers.csv",
    na = c("NA", ".", "", "N/A", "Unknown", "UNKNOWN")) %>%
  janitor::clean_names() %>% 
  separate(
    baker_name, 
    into = c("baker", "last name"), 
    sep = " ") %>% 
  mutate(
    baker = case_match(
      baker,
      "Jo" ~ "Joanne",
      .default = baker
      
    )
  )
  
head(bakers_df)


```

# Reading in and cleaning bakes csv file
```{r}
bakes_df <- 
  read_csv(
    "Local_Data_Files/gbb_datasets/bakes.csv",
    na= c("NA", ".", "", "N/A", "Unknown", "UNKNOWN")) %>% 
  janitor::clean_names() %>%
  relocate(baker) %>% 
  mutate(
    baker = case_match(
      baker,
      '"Jo"' ~ "Joanne",
      .default = baker
    )
  )

head(bakes_df)


```

# Reading in and cleaning results csv
```{r}
results_df <- 
  read_csv(
    "Local_Data_Files/gbb_datasets/results.csv",
    na = c("NA", ".", "", "N/A", "Unknown", "UNKNOWN"),
    skip =2 ) %>% 
  janitor::clean_names() %>%
  relocate(baker)

head(results_df)
```




## Combine the Datasetsand check for Loss of Data
```{r}
gbb_combined <- 
  left_join(
    results_df, bakes_df) %>% 
  left_join(
    bakers_df) %>% 
  janitor::clean_names()

anti_join(bakers_df, gbb_combined)
anti_join(bakes_df, gbb_combined)

```

## organize so that variables are in meaningful order
```{r}
gbb_combined <- gbb_combined %>% 
  relocate(baker, last_name, baker_age, baker_occupation, hometown, series,
    episode, signature_bake, show_stopper, everything()
  )

head(gbb_combined)

```
## Export Data
```{r}
write_csv(gbb_combined,  "Local_Data_Files/gbb_datasets/gbb_combined.csv")
```


## Approach to cleaning 
In order to begin the cleaning process, I first read in each of the csv files 
and used head to see what columns were included. From here, I noticed that 
the bakers.csv originally had first and last name grouped together. In order 
to ensure that baker names would match between the 3 csv files, I separated 
the baker column into 2: (1) Baker, (2) Last Name. Afterwards, I used anti-join
to determine where there was not overlap between each of the 3 dataframes (
this code chunk has since been deleted). I noticed that result.csv had the 
least missing data when anti-joined with the other two, which makes sense, 
since it was the largest dataset. I then did two consecutive left join calls 
on results with the other two dfs. Here, I found that there was still missing 
rows. In particular, it seemed that the missing rows corresponded to different 
variations of the name Joanne. As a result, I standardized all the variations 
of Joanne to Joanne (matching what was found in results.csv). After doing this,
using left_join to make the gbb_combined df and then testing it for loss of
rows with anti-join revealed no missing data. Regarding this "cleaned" version
of the dataset, I still wonder whether I should have gone one step further.
Looking at the gbb_combined.csv file, contestants have entries in the df, even 
after they have been eliminated from the show. This leaves multiple rows filled
with NA for most variables. I did not remove this, as this was the original 
structure in results.csv, but I wonder if should have.


## create reader-friendly table

```{r}
winner_S5_S10 <- gbb_combined %>%
  filter(
    series %in% c(5:10), 
    result == "STAR BAKER", 
    .keepall = TRUE
  ) %>% 
  pivot_wider(
    names_from = c(series, episode), 
    values_from = result, 
    id_cols = c(baker, last_name) 
  ) %>% 
  knitr::kable()

winner_S5_S10

```

Examining the data broadly, we can see that neither success in the early 
season, nor having the most victories, suggested ultimately being the victor. 
This was not the case for season 5, which had a predictable winner in Richard 
Burr as he won 2 episodes early in the season and later would finish with 5
victories. However with season 6, the winner, Nadiya Hussein, did not 
achieve her first victory until the 5th episode, over halfway through the 
season. A similar conclusion can be drawn about season 7 winner Andrew Smith, 
season 8 winner Sophie Faldo, and season 9 winner Ruby Bhogal.Season 10 winner
Alice Fevronia got her first victory in episode 2, but was by no means a 
favorite to win, as Steph Blackwell tallied more total victories during the 
season.


## Read and Clean Viewers.csv

```{r}
viewers_df <- 
  read_csv(
    "Local_Data_Files/gbb_datasets/viewers.csv",
    na= c("NA", ".", "", "N/A", "Unknown", "UNKNOWN")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    series_1:series_10,
    names_to = "series",
    values_to = "views"
  )

head(viewers_df, 10)
  
```
The average viewership during the first season was `r viewers_df %>% filter(
series == "series_1") %>% pull(views) %>% mean(na.rm = TRUE)`. 
The average viewership during the fifth season was 
`r viewers_df %>% filter(series == "series_5") %>% pull(views) %>% 
mean()`. 

Note: I wanted to convert the series column into an integer type by removing 
the series prefix from each observation, but was unsure if we are allowed to 
use functions outside of the scope fo whats taught. I would have used either 
substr (base R) or str_remove (dplyr).

